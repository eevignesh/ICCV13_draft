\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
% Include other packages here, before hyperref.
  
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\DeclareMathOperator*{\argmin}{\arg\!\min}
%\DeclareMathOperator*{\simi}{\sim\!}
\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{183} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
%\title{Talk to the Machine: What can we Learn From Natural Language Video Descriptions?}
\title{Supplementary Section: Video Event Understanding using Natural Language Descriptions}

\author{Vignesh Ramanathan$^*$ \quad \quad Percy Liang$^\dagger$ \quad \quad Li Fei-Fei$^\dagger$ \\
$^*$Department of Electrical Engineering, Stanford University \\
$^\dagger$Computer Science Department, Stanford University\\
{\tt\small \{vigneshr, pliang, feifeili\}@cs.stanford.edu}
}\maketitle
%\thispagestyle{empty}

\renewcommand{\thesection} {\Alph{section}}

\section{Optimization for Posterior Regularization} \label{sec:PR_opt}

The optimization problem solved to learn our model with Posterior Regularization (PR) is shown again in Eq.~\ref{eq:PR_1}. To recount, $\mathbf{a}, \mathbf{r}$ are the action and role assignments to all the human tracklets across all videos and $Q(\mathbf{a}, \mathbf{r})$ is the corresponding probability of the assignments. The CRF log-likelihood of the assignments is given by $L(\mathbf{a}, \mathbf{r}; w)$, where $w$ are the weights to be learnt.

\vspace*{-7pt}
{\small
\begin{equation}\label{eq:PR_1}
\begin{aligned}
& \min \limits_{w,Q, \atop \delta \geq 0, \eta \geq 0} \hspace*{-19pt}
& \hspace*{-14pt} &  \frac{\|w \|^2}{2} \hspace*{-2pt} - \hspace*{-2pt} \mathbb{E}_Q[L(\mathbf{a}, \mathbf{r}; w)] \hspace*{-2pt} +  \hspace*{-2pt} \sum \limits_{i} \biggl\{\sum \limits_{a}  \delta_i^a \hspace*{-3pt} + \hspace*{-3pt} \sum \limits_{r}  \eta_i^r  \biggr\} \hspace*{-3pt} - \hspace*{-3pt} H_{Q}\\
& \text{subject to} \hspace*{-6pt}
& & \mathbb{E}_Q [N_i(a)] \geq 1 - \delta_i^a, \;\; \forall y_i^a \hspace*{-2pt}=\hspace*{-2pt} 1 \\
&\hspace*{-3pt}&& \mathbb{E}_Q [N_i(a)] \leq \delta_i^a, \hspace*{22pt} \forall y_i^a \hspace*{-2pt}=\hspace*{-2pt} -1 \\
&\hspace*{-3pt}&& \mathbb{E}_Q [M_i(r)] \geq 1 - \eta_i^r, \;\; \forall z_i^r \hspace*{-2pt}=\hspace*{-2pt} 1 \\
&\hspace*{-3pt}&& \mathbb{E}_Q [M_i(r)] \leq \eta_i^r, \hspace*{22pt} \forall z_i^r \hspace*{-2pt}=\hspace*{-2pt} -1,
\end{aligned}
\end{equation}} where $H_Q$ is the entropy of $Q$, $N_i(a) = \sum \limits_{h} \mathbf{1}(a_h^i=a)$ and $M_i(r) = \sum \limits_{h} \mathbf{1}(r_h^i=r)$.

In the absence of the slack constraints, Eq.~\ref{eq:PR_1} is traditionally learnt through an Expectation Maximization (EM) procedure. Similarly, we will discuss an EM algorithm with a modified expectation step to account for the constraints. The modified expectation step (E'-step) and the maximization step (M-step) at an iteration $t$ are explained below.

\subsection{E'-step}
The E'-step at an iteration $t$ refers to the optimization of Eq.~\ref{eq:PR_1}, while keeping the model weights fixed from the previous iteration $w^{(t-1)}$. The minimizing $Q$ provides $Q^{(t)}$. The E'-step at iteration $t$ is shown in Eq.~\ref{eq:PR_Estep}.

\vspace*{-7pt}
{\small
\begin{equation}\label{eq:PR_Estep}
\begin{aligned}
& \max \limits_{Q \atop \delta \geq 0, \eta \geq 0} \hspace*{-19pt}
& \hspace*{-14pt} &  \mathbb{E}_Q[L(\mathbf{a}, \mathbf{r}; w^{(t-1)})] \hspace*{-2pt} - \hspace*{-2pt} \sum \limits_{i} \biggl(\sum \limits_{a}  \delta_i^a \hspace*{-3pt} + \hspace*{-3pt} \sum \limits_{r}  \eta_i^r  \biggr) \hspace*{-2pt} + \hspace*{-2pt} H_{Q}\\
& \text{subject to} \hspace*{-6pt}
& & \mathbb{E}_Q [N_i(a)] \geq 1 - \delta_i^a, \;\; \forall y_i^a \hspace*{-2pt}=\hspace*{-2pt} 1 \\
&\hspace*{-3pt}&& \mathbb{E}_Q [N_i(a)] \leq \delta_i^a, \hspace*{22pt} \forall y_i^a \hspace*{-2pt}=\hspace*{-2pt} -1 \\
&\hspace*{-3pt}&& \mathbb{E}_Q [M_i(r)] \geq 1 - \eta_i^r, \;\; \forall z_i^r \hspace*{-2pt}=\hspace*{-2pt} 1 \\
&\hspace*{-3pt}&& \mathbb{E}_Q [M_i(r)] \leq \eta_i^r, \hspace*{22pt} \forall z_i^r \hspace*{-2pt}=\hspace*{-2pt} -1,
\end{aligned}
\end{equation}
}
We notice that the above problem is convex due to the linear nature of constraints arising from our model. The above equation can be solved exactly by minimizing the dual. In order to solve the dual, we first define a modified CRF log-likelihood function $\tilde{L}$ with dual variables $\lambda, \gamma$ as shown in Eq.~\ref{eq:modified_PR}.

{\small 
\begin{eqnarray} \label{eq:modified_PR}
  \tilde{L}(\mathbf{a}, \mathbf{r};w, \lambda, \gamma) & = & \sum \limits_{i=1}^{n} \sum \limits_{h} \biggl( \hspace*{-2pt} w_g(a_i^h, r_i^h) \hspace*{-1pt} \cdot  \hspace*{-1pt} f_g^{x_i} \hspace*{-1pt}  +   \\ \nonumber
  && w_{ac.}(a_i^h) \hspace*{-2pt} \cdot \hspace*{-2pt} f_{ac.}^{h} \hspace*{-1pt} +  \hspace*{-1pt} w_{ro.}(r_i^h) \hspace*{-2pt} \cdot \hspace*{-2pt} f_{ro.}^{h} \\ \nonumber
  && + w_{in.}(a_i^h, r_i^h) \hspace*{-1pt}  + \hspace*{-1pt} \sum_{a}  y_i^a \lambda_i^a \cdot \mathbf{1}(a_i^h = a) + \\ \nonumber
&&  \sum_{r} z_i^r \gamma_i^r \cdot \mathbf{1}(r_i^h = r) \hspace*{-2pt} \biggr) \hspace*{-3pt} - \hspace*{-3pt} \log \tilde{Z}_i(w, \lambda_i, \gamma_i), \nonumber
\end{eqnarray}}where $\tilde{Z}_i$ is the partition function for video $x_i$ corresponding to the modified CRF potential.

We obtain the optimal dual variables $\lambda^*, \gamma^*$ as shown in Eq.~\ref{eq:lambda_gamma}.

{\small
\begin{eqnarray} \label{eq:lambda_gamma}
 \lambda_i^*, \gamma_i^* &=& \argmin \limits_{0 \leq \lambda_i \leq 1 \atop 0 \leq \gamma_i \leq 1} \biggl\{ \log \tilde{Z}_i(w^{(t-1)}, \lambda_i, \gamma_i) -  \\ \nonumber
 && \sum \limits_a \lambda_i^a \hspace*{-2pt} \cdot \hspace*{-2pt} \mathbf{1}(y_i^a = 1) - \sum \limits_r \gamma_i^r \hspace*{-2pt} \cdot \hspace*{-2pt} \mathbf{1}(z_i^r = 1) \biggr\}
\end{eqnarray}
}

Now, $Q^{(t)}(\mathbf{a}, \mathbf{r})$ is obtained by running inference on a CRF whose log-likelihood is given by $\tilde{L}(\mathbf{a}, \mathbf{r}; w^{(t-1)}, \lambda^*, \gamma^*)$. Thus the problem is tractable since we solve Eq.~\ref{eq:lambda_gamma} and run inference separately for each video.

\subsection{M-step}

The M-step is shown in Eq.~\ref{eq:Mstep}.

\begin{equation}\label{eq:Mstep}
 w^{(t)} =  \argmin \limits_{w} \frac{\|w \|^2}{2} \hspace*{-2pt} - \hspace*{-2pt} \mathbb{E}_{Q^{(t)}}[L(\mathbf{a}, \mathbf{r}; w)]
\end{equation}

As seen, the M-step remains the same as the traditional EM procedure and can be solved through coordinate descent.

\section{Self paced learning to handle outliers}
We wish to use the textual features to handle outliers and ensure that only good examples are chosen 
based on the natural language descriptions. Hence, we define a new potential $\phi_d(x,h,t,a,r)$ 
corresponding to the assignment of action $a$ and role $r$ to the human track $h$ in video $x$ with 
textual description $t$ as shown in Eq.~\ref{eq:potential_tracklet}, 
which includes an additional global potential corresponding to 
the textual features $f_d^{t}$ with corresponding weights given by $w_d$.

\begin{eqnarray}\label{eq:potential_tracklet}
  \Phi_d(x, h, t, a, r) & = & w_{g}(a,r) \cdot f_{g}^x + w_{in.}(a,r) \\ \nonumber
                   & & + w_{ac.}(a) \cdot f_{ac.}^h + w_{ro.}(r) \cdot f_{ro.}^h \\ \nonumber
                   & &  w_d(a, r) \cdot f_d^{t} \nonumber
\end{eqnarray}

The new log-likelihood $L_d$ corresponding to this modified potential is shown below.

\begin{eqnarray}\label{eq:prob_assignment}\nonumber
 \hspace*{-10pt} p_d(a_i, r_i; w, w_d) & \hspace*{-8pt}  = \hspace*{-8pt}  & \frac{1}{Z_i^d} \exp \left(\sum \limits_{h \in \mathcal{H}_i} \Phi_d(x_i, h,t_i,a_i^h, r_i^h) \right) \\ \nonumber
  L_d(\mathbf{a}, \mathbf{r}; w, w_d) & = & \sum \limits_{i} \log p_d(a_i, r_i; w, w_d),
\end{eqnarray}where $Z_i^d$ is the modified partition function for the video $x_i$.

The model learning at an iteration of the self-paced scheme is then defined as shown in Eq. \ref{eq:SPNLP}.
{\small
\begin{equation}\label{eq:SPNLP}
\begin{aligned}
& \min \limits_{w, w_d, Q,\mathcal{V} \atop \delta \geq 0, \eta \geq 0} \hspace*{-19pt}
& \hspace*{-14pt} &  \frac{\|w \|^2}{2} \hspace*{-2pt} + \frac{\|w_d\|^2}{2 \sigma_d^2} - \hspace*{-2pt} \mathbb{E}_Q[L_{d}(\mathbf{a}, \mathbf{r}; w, w_d)]  - \hspace*{-3pt} H_{Q}   \\
&&&  + \hspace*{-2pt} \sum \limits_{a} \biggl\{ \sum \limits_{x_i \in \mathcal{V}^a}\delta_i^a - \frac{|\mathcal{V}^a|}{K} \biggr\} \hspace*{-3pt} + \hspace*{-3pt} \sum \limits_{r} \biggl\{ \sum \limits_{x_j \in \mathcal{V}^r} \eta_j^r - \frac{|\mathcal{V}^r|}{K} \biggr\}\hspace*{-3pt}\\
& \text{subject to} \hspace*{-6pt}
& & \mathbb{E}_Q [N_i(a)] \geq 1 - \delta_i^a, \;\; \forall y_i^a \hspace*{-2pt}=\hspace*{-2pt} 1 \\
&\hspace*{-3pt}&& \mathbb{E}_Q [N_i(a)] \leq \delta_i^a, \hspace*{22pt} \forall y_i^a \hspace*{-2pt}=\hspace*{-2pt} -1 \\
&\hspace*{-3pt}&& \mathbb{E}_Q [M_i(r)] \geq 1 - \eta_i^r, \;\; \forall z_i^r \hspace*{-2pt}=\hspace*{-2pt} 1 \\
&\hspace*{-3pt}&& \mathbb{E}_Q [M_i(r)] \leq \eta_i^r, \hspace*{22pt} \forall z_i^r \hspace*{-2pt}=\hspace*{-2pt} -1,
\end{aligned}
\end{equation}}where, $\mathcal{V}^a$ and $\mathcal{V}^r$ are the sets of training examples corresponding to action $a$ and role $r$ respectively, whose corresponding action or role  labels are used at an iteration of the self paced procedure to enforce PR constraints. In our experiments, we initialize $\mathcal{V}$ to be the same as the examples considered for baseline models not using the natural language descriptions.

We run the model for three iterations for the sake of tractable learning and anneal the parameter $\sigma_d$ from $1$ to $0$ in the second iteration. This ensures that the textual features are considered initially, but are neglected midway when the model is more confident. The optimization in Eq. \ref{eq:SPNLP} can be solved approximately through an alternate search strategy, which alternates between choosing the optimal set of examples $\mathcal{V}$ with fixed model weights and optimizing the model weights keeping $\mathcal{V}$ fixed. We refer the reader to \cite{Kumar_NIPS10} for further details on the optimization. $K$ is annealed linearly from $10$ to $2$, so that at the end, the model includes only those examples which are not extreme outliers.

{\small
\bibliographystyle{ieee}
\bibliography{TextActionRoles}
}

\end{document}
